{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $Steepest$ $Descent$ $algorithm$ $(Cauchy$ $Method)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, to find the minimum value of a cost functiong using steepest descent algorithm, we will be using the concept of maxima-minima.\n",
    ">\n",
    "- Hence, we will be exclusively using partial diffentiation.\n",
    ">\n",
    "- In python, we can simply perform diffentiation using SymPy library.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 x^{2} - 3 x y^{3} + 5 y z + 8$"
      ],
      "text/plain": [
       "2*x**2 - 3*x*y**3 + 5*y*z + 8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import *\n",
    "x,y,z = symbols(\"x y z\")\n",
    "f = 2*x**2 - 3*x*y**3 + 5*y*z + 8\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 4 x - 3 y^{3}$"
      ],
      "text/plain": [
       "4*x - 3*y**3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partially diffentiating \"f\" w.r.t \"x\"\n",
    "fx = diff(f,x)\n",
    "fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding value of \"f\" at point (1,1,1)\n",
    "# For this we need to 1st lambdify our function.\n",
    "df = lambdify([x,y,z],f)\n",
    "df(1,1,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic steps:\n",
    "- Start with an arbitrary initial point $X_{1}$.  Set the iteration number as i = 1.\n",
    ">\n",
    "- Find the search direction $S_{i}$ as\n",
    "$S_{i}$ = −∇$f_{i}$ = −∇$f(X_{i})$\n",
    ">\n",
    "- Determine the optimal step length $λ_{i}$ in the direction $S_{i}$ and set \n",
    "$X_{i+1}$ = $X_{i}$ + $λ_{i}$ $S_{i}$\n",
    ">\n",
    "- Test the new point, $X_{i+1}$, for optimality. If $X_{i+1}$ is optimum, stop the process. Otherwise, go to last step.\n",
    ">\n",
    "- Set the new iteration number i = i + 1 and go to step 2\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Iteration 1 : \n",
      "\t Gradient : [1, -1]\n",
      "\t Step size : [-1  1]\n",
      "\t Lamda value : 1\n",
      "\t Point : [-1, 1]\n",
      "\t Value of function 'f' at Point (-1, 1) is -1\n",
      "\n",
      "After Iteration 2 : \n",
      "\t Gradient : [-1, -1]\n",
      "\t Step size : [1 1]\n",
      "\t Lamda value : 1/5\n",
      "\t Point : [-0.8000, 1.2000]\n",
      "\t Value of function 'f' at Point (-0.8000, 1.2000) is -1.2000\n",
      "\n",
      "After Iteration 3 : \n",
      "\t Gradient : [1/5, -1/5]\n",
      "\t Step size : [-1/5 1/5]\n",
      "\t Lamda value : 1\n",
      "\t Point : [-1, 1.4000]\n",
      "\t Value of function 'f' at Point (-1, 1.4000) is -1.2400\n",
      "\n",
      "After Iteration 4 : \n",
      "\t Gradient : [-1/5, -1/5]\n",
      "\t Step size : [1/5 1/5]\n",
      "\t Lamda value : 1/5\n",
      "\t Point : [-0.9600, 1.4400]\n",
      "\t Value of function 'f' at Point (-0.9600, 1.4400) is -1.2480\n",
      "\n",
      "After Iteration 5 : \n",
      "\t Gradient : [1/25, -1/25]\n",
      "\t Step size : [-1/25 1/25]\n",
      "\t Lamda value : 1\n",
      "\t Point : [-1, 1.4800]\n",
      "\t Value of function 'f' at Point (-1, 1.4800) is -1.2496\n",
      "\n",
      "After Iteration 6 : \n",
      "\t Gradient : [-1/25, -1/25]\n",
      "\t Step size : [1/25 1/25]\n",
      "\t Lamda value : 1/5\n",
      "\t Point : [-0.9920, 1.4880]\n",
      "\t Value of function 'f' at Point (-0.9920, 1.4880) is -1.2499\n",
      "\n",
      "After Iteration 7 : \n",
      "\t Gradient : [1/125, -1/125]\n",
      "\t Step size : [-1/125 1/125]\n",
      "\t Lamda value : 1\n",
      "\t Point : [-1, 1.4960]\n",
      "\t Value of function 'f' at Point (-1, 1.4960) is -1.2500\n",
      "\n",
      "After Iteration 8 : \n",
      "\t Gradient : [-1/125, -1/125]\n",
      "\t Step size : [1/125 1/125]\n",
      "\t Lamda value : 1/5\n",
      "\t Point : [-0.9984, 1.4976]\n",
      "\t Value of function 'f' at Point (-0.9984, 1.4976) is -1.2500\n",
      "\n",
      "After Iteration 9 : \n",
      "\t Gradient : [1/625, -1/625]\n",
      "\t Step size : [-1/625 1/625]\n",
      "\t Lamda value : 1\n",
      "\t Point : [-1, 1.4992]\n",
      "\t Value of function 'f' at Point (-1, 1.4992) is -1.2500\n",
      "\n",
      "After Iteration 10 : \n",
      "\t Gradient : [-1/625, -1/625]\n",
      "\t Step size : [1/625 1/625]\n",
      "\t Lamda value : 1/5\n",
      "\t Point : [-0.9997, 1.4995]\n",
      "\t Value of function 'f' at Point (-0.9997, 1.4995) is -1.2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sympy import *\n",
    "\n",
    "x,y = symbols(\"x y\")\n",
    "\n",
    "# Cost Function\n",
    "f = x - y + 2*x**2 + 2*x*y + y**2\n",
    "\n",
    "# Initial point\n",
    "x0 = np.array([0, 0])\n",
    "\n",
    "x_ = x0[0]\n",
    "y_ = x0[1]\n",
    "\n",
    "# Gradients w.r.t x & y\n",
    "fx = diff(f,x)\n",
    "fy = diff(f,y)\n",
    "\n",
    "# Lambdifing Gradients,\n",
    "# i.e. converting into numeric eq.\n",
    "grad_x = lambdify([x,y], fx)\n",
    "grad_y = lambdify([x,y], fy)\n",
    "\n",
    "# No. of iterations\n",
    "epochs = 10\n",
    "\n",
    "nf = lambdify([x,y],f)\n",
    "\n",
    "for i in range(1,epochs+1):\n",
    "    # Compute gradient value at x_ & y_\n",
    "    g_x = grad_x(x_, y_)\n",
    "    g_y = grad_y(x_, y_)\n",
    "\n",
    "    # Search direction 'S' = -Gradients\n",
    "    S = -np.array([g_x,g_y])\n",
    "\n",
    "    # Defining Step size lambda as 'l'\n",
    "    l = symbols(\"l\")\n",
    "    expr = np.array([x_,y_]) + l*S\n",
    "    f_sub = f.subs([(x,expr[0]),(y,expr[1])])\n",
    "    df = diff(f_sub,l)\n",
    "    \n",
    "    # solving equation to get value of lambda\n",
    "    sol = solve(df)\n",
    "    \n",
    "    # Updating x value = old_x + lamda*S\n",
    "    x_new = np.array([x_,y_]) + sol[0]*S\n",
    "    \n",
    "    a = round(x_new[0],4)\n",
    "    b = round(x_new[1],4)\n",
    "    print(f\"After Iteration {i} : \")\n",
    "    print(f\"\\t Gradient : {[g_x, g_y]}\")\n",
    "    print(f\"\\t Step size : {S}\")\n",
    "    print(f\"\\t Lamda value : {sol[0]}\")\n",
    "    print(f\"\\t Point : {[a, b]}\")\n",
    "    print(f\"\\t Value of function 'f' at Point {(a, b)} is {nf(a,b)}\")\n",
    "    print()\n",
    "    x_ = x_new[0]\n",
    "    y_ = x_new[1]\n",
    "\n",
    "# We can repeat this process till the x_new and x_old are approx. same.\n",
    "# Hence, we can set very small tolerance like 0.000001, \n",
    "# so if x_new is less then tolerance, we will break out of loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
